{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Neural Network that learns a double well function. Graphviz is used to visualise the trajectory in weight space.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # different backend so we don't need tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import sys\n",
    "sys.path += ['c:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dblWell(x, bias=0): # double well\n",
    "    E0=1\n",
    "    E1=0.05\n",
    "    center_x = 5\n",
    "    \n",
    "    x = x-center_x\n",
    "    return E0*(E1*x**4-x**2+bias*x**3)\n",
    "\n",
    "def fractalLike(x, L=10): # fractal-like Fourier series from Ann. Stat. 34, 1636\n",
    "    coeffs = [.21, 1.25, .61, .25, .13, .10, 1.16, .18, .12, .23, .21, .19, .37, .99, .36, .02, .06, .08, .09, .04]\n",
    "    series = 0.0\n",
    "    for i,coeff in enumerate(coeffs):\n",
    "        series += coeff*np.sin(i*2.0*np.pi*x/L)\n",
    "    return 2.0*series\n",
    "\n",
    "\n",
    "class singleLayerNet(nn.Module): # from https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(singleLayerNet, self).__init__()\n",
    "        self.hidden = nn.Linear(n_feature, n_hidden)   # single hidden layer with n_hidden output features\n",
    "        self.predict = nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "class threeLayerNet(nn.Module): # from https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(threeLayerNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_feature, n_hidden)   # input layer with n_hidden output features\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden-20)  # single hidden layer with n_hidden output features\n",
    "        n_hidden -= 20 # narrow down the NN\n",
    "        self.hidden3 = nn.Linear(n_hidden, n_hidden)    # single hidden layer with n_hidden output features\n",
    "        self.predict = nn.Linear(n_hidden, n_output)    # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.hidden1(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x    \n",
    "\n",
    "def getCurrentWeights(net, indices): # extract weights of layers in indices in form of a python list from a pytorch net\n",
    "    weightsReturn = []\n",
    "    for index in indices:\n",
    "        layerWeights = list(net.parameters())[index][0] # extract weights from parameter set\n",
    "        layerWeights = layerWeights.data.numpy()        # convert from pytorch tensor to numpy array\n",
    "        #reluWeights = layerWeights.tolist()            # convert from numpy array to python list\n",
    "        weightsReturn.append(np.copy(layerWeights))     # return a copy, since usually net.parameters() gives an iterator reference    \n",
    "    return weightsReturn\n",
    "\n",
    "def getDistanceMatrix(weights, magnify=1, exclude_first=0):\n",
    "    dim = np.shape(weights)[0] - exclude_first\n",
    "    distance = np.zeros((dim, dim))\n",
    "    for t1 in range(exclude_first, dim):\n",
    "        for t2 in range(exclude_first, t1):\n",
    "            # sum over element-wise squared differences\n",
    "            for layer in range(np.shape(weights)[1]):\n",
    "                distance[t1, t2] += sum(np.square(weights[t1][layer]-weights[t2][layer]))*magnify\n",
    "                distance[t2, t1] = distance[t1, t2] # this distance metric is symmetric\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss = 8.2350\n",
      "training loss = 2.1992\n",
      "training loss = 1.7022\n",
      "training loss = 1.3132\n",
      "training loss = 1.8387\n",
      "training loss = 1.0793\n",
      "training loss = 0.9516\n",
      "training loss = 1.0213\n",
      "training loss = 0.9773\n",
      "training loss = 0.8920\n",
      "training loss = 0.8371\n",
      "training loss = 0.8112\n",
      "training loss = 0.7937\n",
      "training loss = 0.7845\n",
      "training loss = 0.7780\n",
      "training loss = 0.7740\n",
      "training loss = 0.8474\n",
      "training loss = 0.7989\n",
      "training loss = 0.7639\n",
      "training loss = 0.9400\n",
      "-------- test loss = 1.4204\n"
     ]
    }
   ],
   "source": [
    "# sample from function\n",
    "N       = 300        # how many total sampled points\n",
    "N_train = int(0.8*N) # how many training points\n",
    "\n",
    "L = 10 # sample points between 0 and this value\n",
    "X = L*np.random.rand(N)\n",
    "\n",
    "def samplingFunction(X):\n",
    "    #return dblWell(X, bias=-5e-2)\n",
    "    return fractalLike(X, L)\n",
    "y = samplingFunction(X) # choose which function to sample from\n",
    "\n",
    "X_train = X[:N_train].reshape(-1,1) # training data\n",
    "y_train = y[:N_train].reshape(-1,1)\n",
    "X_test = X[N_train:].reshape(-1,1)  # validation data\n",
    "y_test = y[N_train:].reshape(-1,1)\n",
    "\n",
    "\n",
    "mode = \"torch\" # torch (pytorch) or tf (tensorflow)\n",
    "progress_plots = False # plot progress during optimisation\n",
    "\n",
    "if (mode == \"tf\"): # tensorflow mode - not fully implemented yet\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "    model = Sequential()\n",
    "    # reshape into 2D before input?\n",
    "    model.add(Dense(3, input_shape=(1,)))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    train_model = model.fit(X_train, y_train,\n",
    "                  batch_size=128,\n",
    "                  epochs=50,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test, y_test))\n",
    "\n",
    "elif (mode == \"torch\"):\n",
    "    n_features = 1\n",
    "    x = torch.from_numpy(X_train).float()\n",
    "    y = torch.from_numpy(y_train).float()\n",
    "\n",
    "    # characterise neural net\n",
    "    H1,H2 = 80,80\n",
    "    layerIndices = [0,2,4] # array indices of linear layers (used later in weight extraction)\n",
    "\n",
    "    net = nn.Sequential(nn.Linear(x.shape[1],H1),    # define the network\n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(H1, H2), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(H2, x.shape[1]))\n",
    "                        #nn.LogSoftmax(dim=1))    \n",
    "    epochs = 20000\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n",
    "    loss_func = nn.MSELoss()  # mean squared loss for regression\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        prediction = net(x)     # input x and predict yhat based on x\n",
    "        loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "        if t % int(epochs/20) == 0:\n",
    "            # show learning process\n",
    "            print('training loss = %.4f' % loss.data.numpy())\n",
    "            if (progress_plots):\n",
    "                plt.cla()\n",
    "                plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "                plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "                plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 15, 'color':  'red'})\n",
    "                plt.savefig('tmp'+str(t)+'.jpg')\n",
    "\n",
    "            # save all current weights for visualisation    \n",
    "            currentWeights = getCurrentWeights(net, layerIndices)\n",
    "            weights.append(currentWeights)\n",
    "\n",
    "    # print loss on test set\n",
    "    xtest = torch.from_numpy(X_test).float()\n",
    "    ytest = torch.from_numpy(y_test).float()\n",
    "    testloss = loss_func(net(xtest), ytest)\n",
    "    print(\"-------- test loss = %.4f\" % testloss.data.numpy())\n",
    "\n",
    "    # calculate distance matrix between the sets of weights for every time step\n",
    "    distance = getDistanceMatrix(weights, magnify=1.5, exclude_first=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == \"torch\"):\n",
    "    # draw distance graph\n",
    "    dt = [('len', float)]\n",
    "    distance = distance.view(dt)\n",
    "    graph = nx.from_numpy_matrix(distance)\n",
    "    graph = nx.drawing.nx_agraph.to_agraph(graph)\n",
    "    graph.node_attr.update(shape=\"circle\", style=\"filled\")\n",
    "    colourmap = matplotlib.cm.get_cmap('jet')\n",
    "    for nodeId in graph.nodes():\n",
    "        node = graph.get_node(nodeId)\n",
    "        colour = matplotlib.colors.to_hex(colourmap(int(nodeId)/graph.number_of_nodes()))\n",
    "        node.attr['fillcolor'] = colour\n",
    "    graph.edge_attr.update(style=\"invis\")  # hide edges\n",
    "    graph.draw('graph.pdf', format='pdf', prog='neato', args=\"-Nfontname=Verdana\")\n",
    "\n",
    "    # draw prediction\n",
    "    plotrange = np.linspace(0, L, num=120)\n",
    "    plt.figure()\n",
    "    plt.plot(plotrange, samplingFunction(plotrange), label='true fct.')\n",
    "    plt.plot(X_train, y_train, '.', label='training data')\n",
    "    plt.plot(X_test, y_test, 'r.', label='test data')\n",
    "    plt.ylim(-10,15)\n",
    "\n",
    "    prediction = net(Variable(torch.Tensor(plotrange.reshape(-1,1))))\n",
    "\n",
    "    plt.plot(plotrange, prediction.data.numpy(), 'k.', label='NN prediction')\n",
    "    plt.legend()\n",
    "    plt.title('Test Loss=%.4f' % testloss)\n",
    "\n",
    "    plt.savefig('prediction.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
